{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation of scrapped data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sommaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Removing for duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- Making a Join on comments with posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- Add a feature sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- Exporting dataframe with all comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6- Making calculation by post (grouping by information of comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "import warnings\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "en = spacy.load('en_core_web_sm')\n",
    "import datetime\n",
    "\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = 'france'\n",
    "update = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments & posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading file =  ../scrapping/exports/france/comments\\france_comments_x67dzf.csv\n",
      "\n",
      "Reading file =  ../scrapping/exports/france/comments\\france_comments_x6ppsd.csv\n",
      "\n",
      "Reading file =  ../scrapping/exports/france/comments\\france_comments_xci5m1.csv\n",
      "\n",
      "Reading file =  ../scrapping/exports/france/comments\\france_comments_xhtzdy.csv\n",
      "\n",
      "Reading file =  ../scrapping/exports/france/comments\\france_comments_yg3dj6.csv\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# getting csv files from the folder\n",
    "path = \"../scrapping/exports/\" + subreddit + \"/comments\"\n",
    "\n",
    "# read all the files with extension .csv\n",
    "filenames = glob.glob(path + \"\\*.csv\")\n",
    "all_comments = pd.DataFrame()\n",
    "# for loop to iterate all csv files\n",
    "for file in filenames:\n",
    "   # reading csv files\n",
    "   print(\"\\nReading file = \",file)\n",
    "   all_comments = all_comments.append(pd.read_csv(file))\n",
    "\n",
    "all_comments = all_comments.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File names: ['../scrapping/exports/france/posts\\\\france_20220901_20221030.csv']\n",
      "\n",
      "Reading file =  ../scrapping/exports/france/posts\\france_20220901_20221030.csv\n"
     ]
    }
   ],
   "source": [
    "# getting csv files from the folder\n",
    "path = \"../scrapping/exports/\" + subreddit + \"/posts\"\n",
    "\n",
    "# read all the files with extension .csv\n",
    "filenames = glob.glob(path + \"\\*.csv\")\n",
    "print('File names:', filenames)\n",
    "all_titres = pd.DataFrame()\n",
    "# for loop to iterate all csv files\n",
    "for file in filenames:\n",
    "   # reading csv files\n",
    "   print(\"\\nReading file = \",file)\n",
    "   all_titres = all_titres.append(pd.read_csv(file))\n",
    "\n",
    "all_titres = all_titres.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_df = pd.read_parquet('exports/' + subreddit + '/' + subreddit + '_comments_merged.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments = all_comments.drop_duplicates()\n",
    "all_comments.rename(columns = {'commentId':'comment_id', 'parent_commentId':'parent_comment_id'}, inplace = True)\n",
    "\n",
    "all_titres = all_titres.drop_duplicates()\n",
    "all_titres.rename(columns = {'postId':'post_id'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if we still have duplicated on the IDs of both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(all_comments[all_comments.duplicated(['comment_id'])]) == 0, \"Meh, I found some duplicated comments IDs in the dataframe\"\n",
    "assert len(all_titres[all_titres.duplicated(['post_id'])]) == 0, \"Meh, I found some duplicated post IDs in the dataframe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining comments & posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = all_titres.merge(all_comments, on=\"post_id\", how = \"left\", suffixes=['_post', '_comment'])\n",
    "columns_name = {'authors':'author_comment',\n",
    "    'author':'author_post',\n",
    "    'body':'text_post',\n",
    "    'text':'text_comment'}\n",
    "comments.rename(columns = columns_name, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess \n",
    "### Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['created_comment'] = pd.to_datetime(comments['created_comment'], format= '%Y/%m/%d')\n",
    "comments['created_post'] = pd.to_datetime(comments['created_post'], format= '%Y/%m/%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['year_comment']= comments['created_comment'].dt.year\n",
    "comments['month_comment']= comments['created_comment'].dt.month\n",
    "comments['day_comment']= comments['created_comment'].dt.day\n",
    "comments['year_post']= comments['created_post'].dt.year\n",
    "comments['month_post']= comments['created_post'].dt.month\n",
    "comments['day_post']= comments['created_post'].dt.day\n",
    "del comments['created_comment']\n",
    "del comments['created_post']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a feature sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a- Construction of a NLP pipeline to clean the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_pipeline(comment) -> str:\n",
    "    comment = str(comment).lower()\n",
    "    comment = comment.replace('\\n', ' ').replace('\\r', '')\n",
    "    comment = ' '.join(comment.split())\n",
    "    comment = re.sub(r\"[A-Za-z\\.]*[0-9]+[A-Za-z%°\\.]*\", \"\", comment)\n",
    "    comment = re.sub(r\"(\\s\\-\\s|-$)\", \"\", comment)\n",
    "    comment = re.sub(r\"[,\\!\\?\\%\\(\\)\\/\\\"]\", \"\", comment)\n",
    "    comment = re.sub(r\"\\&\\S*\\s\", \"\", comment)\n",
    "    comment = re.sub(r\"\\&\", \"\", comment)\n",
    "    comment = re.sub(r\"\\+\", \"\", comment)\n",
    "    comment = re.sub(r\"\\#\", \"\", comment)\n",
    "    comment = re.sub(r\"\\$\", \"\", comment)\n",
    "    comment = re.sub(r\"\\£\", \"\", comment)\n",
    "    comment = re.sub(r\"\\%\", \"\", comment)\n",
    "    comment = re.sub(r\"\\:\", \"\", comment)\n",
    "    comment = re.sub(r\"\\@\", \"\", comment)\n",
    "    comment = re.sub(r\"\\-\", \"\", comment)\n",
    "\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_fr = set(STOP_WORDS)\n",
    "deselect_stop_words = ['ne','pas','plus','personne','aucun','ni','aucune','rien']\n",
    "for w in deselect_stop_words:\n",
    "    if w in stop_words_fr:\n",
    "        stop_words_fr.remove(w)\n",
    "    else:\n",
    "        continue\n",
    "stop_words_en = en.Defaults.stop_words\n",
    "stop_words = stop_words_fr.union(stop_words_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying pipeline and removing stopwords from our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments['text_processed'] = comments['text_comment'].apply(nlp_pipeline)\n",
    "comments['text_processed'] = comments['text_processed'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "comments['title_processed'] = comments['title'].apply(nlp_pipeline)\n",
    "comments['title_processed'] = comments['title_processed'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the commentBlob library to get sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "\n",
    "senti_num_list = []\n",
    "senti_cat_list = []\n",
    "for i in comments[\"text_processed\"]:\n",
    "    vs = tb(i).sentiment[0]\n",
    "    senti_num_list.append(vs)\n",
    "    if (vs > 0.08):\n",
    "        senti_cat_list.append('Positive')\n",
    "    elif (vs < -0.08):\n",
    "        senti_cat_list.append('Negative')\n",
    "    else:\n",
    "        senti_cat_list.append('Neutral')\n",
    "\n",
    "comments['sentiment_num'] = senti_num_list\n",
    "comments['sentiment_cat'] = senti_cat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Original comment ##\n",
      "Oui et le crime de génocide ont été jugés par le TPI-R pour le Rwanda et le TPI-Y pour les crimes durant l'ex-Yougoslavie (Srbrenica a été un élément déclencheur). Ce n'était pas parfait notamment pour le Rwanda où beaucoup de responsables n'ont pas été jugés mais Carla Del Ponte a fait un boulot énorme.\n",
      "\n",
      "Elle est partie après avoir bataillé pour tenter de stopper les atrocités de la guerre en Syrie mais le Conseil de Sécurité de l'ONU est trop puissant, avec son fonctionnement des États membres avec droit de veto.\n",
      "## Score comment ##\n",
      "0.11\n"
     ]
    }
   ],
   "source": [
    "print('## Original comment ##')\n",
    "print(comments['text_comment'][100])\n",
    "print('## Score comment ##')\n",
    "print(comments['sentiment_num'][100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posts aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_22204\\1013376857.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  posts_no_com['sentiment_mean'] = 0\n",
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_22204\\1013376857.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  posts_no_com['nb_comment'] = 0\n"
     ]
    }
   ],
   "source": [
    "# We separate posts we no comments for later calculations and set sentiment_mean and nb_comment to 0.\n",
    "posts_no_com = comments[comments['comment_id'].isna()]\n",
    "posts_no_com['sentiment_mean'] = 0\n",
    "posts_no_com['nb_comment'] = 0\n",
    "assert len(posts_no_com[posts_no_com.duplicated(['post_id'])]) == 0, \"Meh, I found some duplicated post IDs in the dataframe\"\n",
    "\n",
    "#We drop rows of posts where we had 0 comment, we had to keep it before to calculate the number of comments per posts.\n",
    "comments = comments[comments['comment_id'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouping by all columns and calculating mean of metrics\n",
    "posts = comments.groupby(['post_id','title', 'text_post', 'url', 'author_post', 'permalink_post','flair', 'year_post', 'month_post', 'day_post', 'title_processed'], as_index=False, dropna=False).agg(\n",
    "            {\n",
    "                'sentiment_num':'mean',\n",
    "                'comment_id':'size'\n",
    "            })\n",
    "posts[\"comment_id\"] = posts[\"comment_id\"].astype('int64')\n",
    "#We now append with posts with no comment\n",
    "posts = posts.append(posts_no_com[posts.columns], ignore_index=True)\n",
    "assert len(posts) == len(all_titres), \"We don't have the same amount of posts than at the beginning.\"\n",
    "\n",
    "posts.rename(columns = {'sentiment_num':'sentiment_mean','comment_id':'nb_comment'}, inplace = True)\n",
    "#Append nb_comment is Nan so we're filling with 0\n",
    "posts['nb_comment'] = posts['nb_comment'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved csv df in : exports/france/france_comments_merged.csv\n",
      "Saved parquet df in : exports/france/france_comments_merged.parquet\n",
      "Saved csv df in : exports/france/france_posts_merged.csv\n",
      "Saved parquet df in : exports/france/france_posts_merged.parquet\n"
     ]
    }
   ],
   "source": [
    "def save_comments_transform(df:pd.DataFrame):\n",
    "    file_name = subreddit + '_comments_merged'\n",
    "    path_csv = 'exports/france/' + file_name + '.csv'\n",
    "    df.to_csv(path_csv, index = False, encoding = 'utf-8')\n",
    "    print('Saved csv df in : ' + path_csv)\n",
    "    path_parquet = 'exports/france/' + file_name + '.parquet'\n",
    "    df.to_parquet(path_parquet, index = False, engine='pyarrow')\n",
    "    print('Saved parquet df in : ' + path_parquet)\n",
    "\n",
    "def save_posts_transform(df:pd.DataFrame):\n",
    "    file_name = subreddit + '_posts_merged'\n",
    "    path_csv = 'exports/france/' + file_name + '.csv'\n",
    "    df.to_csv(path_csv, index = False, encoding = 'utf-8')\n",
    "    print('Saved csv df in : ' + path_csv)\n",
    "    path_parquet = 'exports/france/' + file_name + '.parquet'\n",
    "    df.to_parquet(path_parquet, index = False, engine='pyarrow')\n",
    "    print('Saved parquet df in : ' + path_parquet)\n",
    "\n",
    "save_comments_transform(comments)\n",
    "save_posts_transform(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>title</th>\n",
       "      <th>text_post</th>\n",
       "      <th>url</th>\n",
       "      <th>author_post</th>\n",
       "      <th>permalink_post</th>\n",
       "      <th>flair</th>\n",
       "      <th>year_post</th>\n",
       "      <th>month_post</th>\n",
       "      <th>day_post</th>\n",
       "      <th>title_processed</th>\n",
       "      <th>sentiment_mean</th>\n",
       "      <th>nb_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x2q6xn</td>\n",
       "      <td>Toi qui liras ça, si tu as un ami fidèle</td>\n",
       "      <td>ne le laisse jamais tomber.  \\n\\nSi cet ami te...</td>\n",
       "      <td>https://www.reddit.com/r/france/comments/x2q6x...</td>\n",
       "      <td>JeuDeLaVie</td>\n",
       "      <td>/r/france/comments/x2q6xn/toi_qui_liras_ça_si_...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>liras ami fidèle</td>\n",
       "      <td>0.061429</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x2qxry</td>\n",
       "      <td>[THREAD] - Comment refaire de la France une su...</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>https://www.reddit.com/r/france/comments/x2qxr...</td>\n",
       "      <td>Wonderful-Excuse4922</td>\n",
       "      <td>/r/france/comments/x2qxry/thread_comment_refai...</td>\n",
       "      <td>Économie</td>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>[thread]comment refaire france superpuissance ...</td>\n",
       "      <td>0.095460</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  post_id                                              title  \\\n",
       "0  x2q6xn           Toi qui liras ça, si tu as un ami fidèle   \n",
       "1  x2qxry  [THREAD] - Comment refaire de la France une su...   \n",
       "\n",
       "                                           text_post  \\\n",
       "0  ne le laisse jamais tomber.  \\n\\nSi cet ami te...   \n",
       "1                                          [removed]   \n",
       "\n",
       "                                                 url           author_post  \\\n",
       "0  https://www.reddit.com/r/france/comments/x2q6x...            JeuDeLaVie   \n",
       "1  https://www.reddit.com/r/france/comments/x2qxr...  Wonderful-Excuse4922   \n",
       "\n",
       "                                      permalink_post     flair  year_post  \\\n",
       "0  /r/france/comments/x2q6xn/toi_qui_liras_ça_si_...       NaN       2022   \n",
       "1  /r/france/comments/x2qxry/thread_comment_refai...  Économie       2022   \n",
       "\n",
       "   month_post  day_post                                    title_processed  \\\n",
       "0           9         1                                   liras ami fidèle   \n",
       "1           9         1  [thread]comment refaire france superpuissance ...   \n",
       "\n",
       "   sentiment_mean  nb_comment  \n",
       "0        0.061429           7  \n",
       "1        0.095460          39  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "211f0b0304102da3630068b06045e4e4a78ffa0a6c596be87323c896afe13624"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
