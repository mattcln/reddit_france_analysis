{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation of scrapped data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sommaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Removing for duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- Making a Join on comments with posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4- Add a feature sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- Exporting dataframe with all comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6- Making calculation by post (grouping by information of comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "import datetime\n",
    "\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = 'france'\n",
    "update = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments & posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading file =  ../scrapping/exports/france/comments\\france_comments_x67dzf.csv\n",
      "\n",
      "Reading file =  ../scrapping/exports/france/comments\\france_comments_x6ppsd.csv\n",
      "\n",
      "Reading file =  ../scrapping/exports/france/comments\\france_comments_xci5m1.csv\n",
      "\n",
      "Reading file =  ../scrapping/exports/france/comments\\france_comments_xhtzdy.csv\n",
      "\n",
      "Reading file =  ../scrapping/exports/france/comments\\france_comments_yg3dj6.csv\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# getting csv files from the folder\n",
    "path = \"../scrapping/exports/\" + subreddit + \"/comments\"\n",
    "\n",
    "# read all the files with extension .csv\n",
    "filenames = glob.glob(path + \"\\*.csv\")\n",
    "all_comments = pd.DataFrame()\n",
    "# for loop to iterate all csv files\n",
    "for file in filenames:\n",
    "   # reading csv files\n",
    "   print(\"\\nReading file = \",file)\n",
    "   all_comments = all_comments.append(pd.read_csv(file))\n",
    "\n",
    "all_comments = all_comments.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File names: ['../scrapping/exports/france/posts\\\\france_20220901_20221030.csv']\n",
      "\n",
      "Reading file =  ../scrapping/exports/france/posts\\france_20220901_20221030.csv\n"
     ]
    }
   ],
   "source": [
    "# getting csv files from the folder\n",
    "path = \"../scrapping/exports/\" + subreddit + \"/posts\"\n",
    "\n",
    "# read all the files with extension .csv\n",
    "filenames = glob.glob(path + \"\\*.csv\")\n",
    "print('File names:', filenames)\n",
    "all_titres = pd.DataFrame()\n",
    "# for loop to iterate all csv files\n",
    "for file in filenames:\n",
    "   # reading csv files\n",
    "   print(\"\\nReading file = \",file)\n",
    "   all_titres = all_titres.append(pd.read_csv(file))\n",
    "\n",
    "all_titres = all_titres.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_df = pd.read_parquet('exports/' + subreddit + '/' + subreddit + '_merged.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments = all_comments.drop_duplicates()\n",
    "all_comments.rename(columns = {'commentId':'comment_id', 'parent_commentId':'parent_comment_id'}, inplace = True)\n",
    "\n",
    "all_titres = all_titres.drop_duplicates()\n",
    "all_titres.rename(columns = {'postId':'post_id'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking if we still have duplicated on the IDs of both dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(all_comments[all_comments.duplicated(['comment_id'])]) == 0, \"Meh, I found some duplicated comments IDs in the dataframe\"\n",
    "assert len(all_titres[all_titres.duplicated(['post_id'])]) == 0, \"Meh, I found some duplicated post IDs in the dataframe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining comments & posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>author_comment</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>parent_comment_id</th>\n",
       "      <th>created_x</th>\n",
       "      <th>permalink_comment</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>url</th>\n",
       "      <th>author_post</th>\n",
       "      <th>created_post</th>\n",
       "      <th>permalink_post</th>\n",
       "      <th>flair</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x5yqb5</td>\n",
       "      <td>Camulogene</td>\n",
       "      <td>in3xqt1</td>\n",
       "      <td>√áa doit √™tre le retour des transhumances</td>\n",
       "      <td>t3_x5yqb5</td>\n",
       "      <td>2022-09-05 00:00:33</td>\n",
       "      <td>/r/france/comments/x5yqb5/rant_mes_troupeaux_d...</td>\n",
       "      <td>RANT : Mes troupeaux de motard sur les routes ...</td>\n",
       "      <td>Je suis en randonn√©e dans le Jura en ce moment...</td>\n",
       "      <td>https://www.reddit.com/r/france/comments/x5yqb...</td>\n",
       "      <td>anyatrans</td>\n",
       "      <td>2022-09-04 23:47:20</td>\n",
       "      <td>/r/france/comments/x5yqb5/rant_mes_troupeaux_d...</td>\n",
       "      <td>Forum Libre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x5yqb5</td>\n",
       "      <td>RobotSpaceBear</td>\n",
       "      <td>in3z4bu</td>\n",
       "      <td>\"Des gens que je ne connais pas aiment quelque...</td>\n",
       "      <td>t3_x5yqb5</td>\n",
       "      <td>2022-09-05 00:10:48</td>\n",
       "      <td>/r/france/comments/x5yqb5/rant_mes_troupeaux_d...</td>\n",
       "      <td>RANT : Mes troupeaux de motard sur les routes ...</td>\n",
       "      <td>Je suis en randonn√©e dans le Jura en ce moment...</td>\n",
       "      <td>https://www.reddit.com/r/france/comments/x5yqb...</td>\n",
       "      <td>anyatrans</td>\n",
       "      <td>2022-09-04 23:47:20</td>\n",
       "      <td>/r/france/comments/x5yqb5/rant_mes_troupeaux_d...</td>\n",
       "      <td>Forum Libre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x5yqb5</td>\n",
       "      <td>la_mine_de_plomb</td>\n",
       "      <td>in3zclt</td>\n",
       "      <td>üòï</td>\n",
       "      <td>t3_x5yqb5</td>\n",
       "      <td>2022-09-05 00:12:31</td>\n",
       "      <td>/r/france/comments/x5yqb5/rant_mes_troupeaux_d...</td>\n",
       "      <td>RANT : Mes troupeaux de motard sur les routes ...</td>\n",
       "      <td>Je suis en randonn√©e dans le Jura en ce moment...</td>\n",
       "      <td>https://www.reddit.com/r/france/comments/x5yqb...</td>\n",
       "      <td>anyatrans</td>\n",
       "      <td>2022-09-04 23:47:20</td>\n",
       "      <td>/r/france/comments/x5yqb5/rant_mes_troupeaux_d...</td>\n",
       "      <td>Forum Libre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x5yqb5</td>\n",
       "      <td>anyatrans</td>\n",
       "      <td>in40qav</td>\n",
       "      <td>Ah non on peut aimer ce qu'on veut a condition...</td>\n",
       "      <td>t1_in3z4bu</td>\n",
       "      <td>2022-09-05 00:22:59</td>\n",
       "      <td>/r/france/comments/x5yqb5/rant_mes_troupeaux_d...</td>\n",
       "      <td>RANT : Mes troupeaux de motard sur les routes ...</td>\n",
       "      <td>Je suis en randonn√©e dans le Jura en ce moment...</td>\n",
       "      <td>https://www.reddit.com/r/france/comments/x5yqb...</td>\n",
       "      <td>anyatrans</td>\n",
       "      <td>2022-09-04 23:47:20</td>\n",
       "      <td>/r/france/comments/x5yqb5/rant_mes_troupeaux_d...</td>\n",
       "      <td>Forum Libre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x5yqb5</td>\n",
       "      <td>quatruplesec</td>\n",
       "      <td>in43n07</td>\n",
       "      <td>ben t'es pas assez loin des routes de passage</td>\n",
       "      <td>t3_x5yqb5</td>\n",
       "      <td>2022-09-05 00:45:39</td>\n",
       "      <td>/r/france/comments/x5yqb5/rant_mes_troupeaux_d...</td>\n",
       "      <td>RANT : Mes troupeaux de motard sur les routes ...</td>\n",
       "      <td>Je suis en randonn√©e dans le Jura en ce moment...</td>\n",
       "      <td>https://www.reddit.com/r/france/comments/x5yqb...</td>\n",
       "      <td>anyatrans</td>\n",
       "      <td>2022-09-04 23:47:20</td>\n",
       "      <td>/r/france/comments/x5yqb5/rant_mes_troupeaux_d...</td>\n",
       "      <td>Forum Libre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204235</th>\n",
       "      <td>yg3dj6</td>\n",
       "      <td>pousse_tes_fesses</td>\n",
       "      <td>iu81r1y</td>\n",
       "      <td>r/france qui rage quand une image leur rappell...</td>\n",
       "      <td>t3_yg3dj6</td>\n",
       "      <td>2022-10-29 09:39:55</td>\n",
       "      <td>/r/france/comments/yg3dj6/besoin_daide_dans_le...</td>\n",
       "      <td>besoin d'aide dans les Landes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.redd.it/yz0l1eq36ow91.jpg</td>\n",
       "      <td>Maxipmz</td>\n",
       "      <td>2022-10-29 01:06:42</td>\n",
       "      <td>/r/france/comments/yg3dj6/besoin_daide_dans_le...</td>\n",
       "      <td>Actus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204236</th>\n",
       "      <td>yg3dj6</td>\n",
       "      <td>Maxipmz</td>\n",
       "      <td>iu86ukc</td>\n",
       "      <td>Je suis d√©sol√© mais moi j'arrive √† lire</td>\n",
       "      <td>t3_yg3dj6</td>\n",
       "      <td>2022-10-29 10:56:32</td>\n",
       "      <td>/r/france/comments/yg3dj6/besoin_daide_dans_le...</td>\n",
       "      <td>besoin d'aide dans les Landes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.redd.it/yz0l1eq36ow91.jpg</td>\n",
       "      <td>Maxipmz</td>\n",
       "      <td>2022-10-29 01:06:42</td>\n",
       "      <td>/r/france/comments/yg3dj6/besoin_daide_dans_le...</td>\n",
       "      <td>Actus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204237</th>\n",
       "      <td>yg3dj6</td>\n",
       "      <td>Maxipmz</td>\n",
       "      <td>iu86xeq</td>\n",
       "      <td>MERCI</td>\n",
       "      <td>t1_iu7vq9b</td>\n",
       "      <td>2022-10-29 10:57:43</td>\n",
       "      <td>/r/france/comments/yg3dj6/besoin_daide_dans_le...</td>\n",
       "      <td>besoin d'aide dans les Landes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.redd.it/yz0l1eq36ow91.jpg</td>\n",
       "      <td>Maxipmz</td>\n",
       "      <td>2022-10-29 01:06:42</td>\n",
       "      <td>/r/france/comments/yg3dj6/besoin_daide_dans_le...</td>\n",
       "      <td>Actus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204238</th>\n",
       "      <td>yg3dj6</td>\n",
       "      <td>Maxipmz</td>\n",
       "      <td>iu86yb6</td>\n",
       "      <td>Exact</td>\n",
       "      <td>t1_iu81r1y</td>\n",
       "      <td>2022-10-29 10:58:05</td>\n",
       "      <td>/r/france/comments/yg3dj6/besoin_daide_dans_le...</td>\n",
       "      <td>besoin d'aide dans les Landes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.redd.it/yz0l1eq36ow91.jpg</td>\n",
       "      <td>Maxipmz</td>\n",
       "      <td>2022-10-29 01:06:42</td>\n",
       "      <td>/r/france/comments/yg3dj6/besoin_daide_dans_le...</td>\n",
       "      <td>Actus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204239</th>\n",
       "      <td>yg3dj6</td>\n",
       "      <td>Protekop</td>\n",
       "      <td>iu8ug2h</td>\n",
       "      <td>√áa a beaucoup chang√© Reddit. Beaucoup de nouve...</td>\n",
       "      <td>t1_iu7yt2z</td>\n",
       "      <td>2022-10-29 15:36:59</td>\n",
       "      <td>/r/france/comments/yg3dj6/besoin_daide_dans_le...</td>\n",
       "      <td>besoin d'aide dans les Landes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://i.redd.it/yz0l1eq36ow91.jpg</td>\n",
       "      <td>Maxipmz</td>\n",
       "      <td>2022-10-29 01:06:42</td>\n",
       "      <td>/r/france/comments/yg3dj6/besoin_daide_dans_le...</td>\n",
       "      <td>Actus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204240 rows √ó 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       post_id     author_comment comment_id  \\\n",
       "0       x5yqb5         Camulogene    in3xqt1   \n",
       "1       x5yqb5     RobotSpaceBear    in3z4bu   \n",
       "2       x5yqb5   la_mine_de_plomb    in3zclt   \n",
       "3       x5yqb5          anyatrans    in40qav   \n",
       "4       x5yqb5       quatruplesec    in43n07   \n",
       "...        ...                ...        ...   \n",
       "204235  yg3dj6  pousse_tes_fesses    iu81r1y   \n",
       "204236  yg3dj6            Maxipmz    iu86ukc   \n",
       "204237  yg3dj6            Maxipmz    iu86xeq   \n",
       "204238  yg3dj6            Maxipmz    iu86yb6   \n",
       "204239  yg3dj6           Protekop    iu8ug2h   \n",
       "\n",
       "                                                     text parent_comment_id  \\\n",
       "0                √áa doit √™tre le retour des transhumances         t3_x5yqb5   \n",
       "1       \"Des gens que je ne connais pas aiment quelque...         t3_x5yqb5   \n",
       "2                                                       üòï         t3_x5yqb5   \n",
       "3       Ah non on peut aimer ce qu'on veut a condition...        t1_in3z4bu   \n",
       "4           ben t'es pas assez loin des routes de passage         t3_x5yqb5   \n",
       "...                                                   ...               ...   \n",
       "204235  r/france qui rage quand une image leur rappell...         t3_yg3dj6   \n",
       "204236            Je suis d√©sol√© mais moi j'arrive √† lire         t3_yg3dj6   \n",
       "204237                                              MERCI        t1_iu7vq9b   \n",
       "204238                                              Exact        t1_iu81r1y   \n",
       "204239  √áa a beaucoup chang√© Reddit. Beaucoup de nouve...        t1_iu7yt2z   \n",
       "\n",
       "                  created_x  \\\n",
       "0       2022-09-05 00:00:33   \n",
       "1       2022-09-05 00:10:48   \n",
       "2       2022-09-05 00:12:31   \n",
       "3       2022-09-05 00:22:59   \n",
       "4       2022-09-05 00:45:39   \n",
       "...                     ...   \n",
       "204235  2022-10-29 09:39:55   \n",
       "204236  2022-10-29 10:56:32   \n",
       "204237  2022-10-29 10:57:43   \n",
       "204238  2022-10-29 10:58:05   \n",
       "204239  2022-10-29 15:36:59   \n",
       "\n",
       "                                        permalink_comment  \\\n",
       "0       /r/france/comments/x5yqb5/rant_mes_troupeaux_d...   \n",
       "1       /r/france/comments/x5yqb5/rant_mes_troupeaux_d...   \n",
       "2       /r/france/comments/x5yqb5/rant_mes_troupeaux_d...   \n",
       "3       /r/france/comments/x5yqb5/rant_mes_troupeaux_d...   \n",
       "4       /r/france/comments/x5yqb5/rant_mes_troupeaux_d...   \n",
       "...                                                   ...   \n",
       "204235  /r/france/comments/yg3dj6/besoin_daide_dans_le...   \n",
       "204236  /r/france/comments/yg3dj6/besoin_daide_dans_le...   \n",
       "204237  /r/france/comments/yg3dj6/besoin_daide_dans_le...   \n",
       "204238  /r/france/comments/yg3dj6/besoin_daide_dans_le...   \n",
       "204239  /r/france/comments/yg3dj6/besoin_daide_dans_le...   \n",
       "\n",
       "                                                    title  \\\n",
       "0       RANT : Mes troupeaux de motard sur les routes ...   \n",
       "1       RANT : Mes troupeaux de motard sur les routes ...   \n",
       "2       RANT : Mes troupeaux de motard sur les routes ...   \n",
       "3       RANT : Mes troupeaux de motard sur les routes ...   \n",
       "4       RANT : Mes troupeaux de motard sur les routes ...   \n",
       "...                                                   ...   \n",
       "204235                      besoin d'aide dans les Landes   \n",
       "204236                      besoin d'aide dans les Landes   \n",
       "204237                      besoin d'aide dans les Landes   \n",
       "204238                      besoin d'aide dans les Landes   \n",
       "204239                      besoin d'aide dans les Landes   \n",
       "\n",
       "                                                     body  \\\n",
       "0       Je suis en randonn√©e dans le Jura en ce moment...   \n",
       "1       Je suis en randonn√©e dans le Jura en ce moment...   \n",
       "2       Je suis en randonn√©e dans le Jura en ce moment...   \n",
       "3       Je suis en randonn√©e dans le Jura en ce moment...   \n",
       "4       Je suis en randonn√©e dans le Jura en ce moment...   \n",
       "...                                                   ...   \n",
       "204235                                                NaN   \n",
       "204236                                                NaN   \n",
       "204237                                                NaN   \n",
       "204238                                                NaN   \n",
       "204239                                                NaN   \n",
       "\n",
       "                                                      url author_post  \\\n",
       "0       https://www.reddit.com/r/france/comments/x5yqb...   anyatrans   \n",
       "1       https://www.reddit.com/r/france/comments/x5yqb...   anyatrans   \n",
       "2       https://www.reddit.com/r/france/comments/x5yqb...   anyatrans   \n",
       "3       https://www.reddit.com/r/france/comments/x5yqb...   anyatrans   \n",
       "4       https://www.reddit.com/r/france/comments/x5yqb...   anyatrans   \n",
       "...                                                   ...         ...   \n",
       "204235                https://i.redd.it/yz0l1eq36ow91.jpg     Maxipmz   \n",
       "204236                https://i.redd.it/yz0l1eq36ow91.jpg     Maxipmz   \n",
       "204237                https://i.redd.it/yz0l1eq36ow91.jpg     Maxipmz   \n",
       "204238                https://i.redd.it/yz0l1eq36ow91.jpg     Maxipmz   \n",
       "204239                https://i.redd.it/yz0l1eq36ow91.jpg     Maxipmz   \n",
       "\n",
       "               created_post  \\\n",
       "0       2022-09-04 23:47:20   \n",
       "1       2022-09-04 23:47:20   \n",
       "2       2022-09-04 23:47:20   \n",
       "3       2022-09-04 23:47:20   \n",
       "4       2022-09-04 23:47:20   \n",
       "...                     ...   \n",
       "204235  2022-10-29 01:06:42   \n",
       "204236  2022-10-29 01:06:42   \n",
       "204237  2022-10-29 01:06:42   \n",
       "204238  2022-10-29 01:06:42   \n",
       "204239  2022-10-29 01:06:42   \n",
       "\n",
       "                                           permalink_post        flair  \n",
       "0       /r/france/comments/x5yqb5/rant_mes_troupeaux_d...  Forum Libre  \n",
       "1       /r/france/comments/x5yqb5/rant_mes_troupeaux_d...  Forum Libre  \n",
       "2       /r/france/comments/x5yqb5/rant_mes_troupeaux_d...  Forum Libre  \n",
       "3       /r/france/comments/x5yqb5/rant_mes_troupeaux_d...  Forum Libre  \n",
       "4       /r/france/comments/x5yqb5/rant_mes_troupeaux_d...  Forum Libre  \n",
       "...                                                   ...          ...  \n",
       "204235  /r/france/comments/yg3dj6/besoin_daide_dans_le...        Actus  \n",
       "204236  /r/france/comments/yg3dj6/besoin_daide_dans_le...        Actus  \n",
       "204237  /r/france/comments/yg3dj6/besoin_daide_dans_le...        Actus  \n",
       "204238  /r/france/comments/yg3dj6/besoin_daide_dans_le...        Actus  \n",
       "204239  /r/france/comments/yg3dj6/besoin_daide_dans_le...        Actus  \n",
       "\n",
       "[204240 rows x 14 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = all_comments.merge(all_titres, on=\"post_id\", how = \"left\")\n",
    "merged_df.rename(columns = {'authors':'author_comment','author':'author_post', 'created_y':'created_post', 'permalink_x':'permalink_comment', 'permalink_y':'permalink_post'}, inplace = True)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Add a feature sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a- Construction of a NLP pipeline to clean the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nlp_pipeline(comment) -> str:\n",
    "    comment = str(comment).lower()\n",
    "    comment = comment.replace('\\n', ' ').replace('\\r', '')\n",
    "    comment = ' '.join(comment.split())\n",
    "    comment = re.sub(r\"[A-Za-z\\.]*[0-9]+[A-Za-z%¬∞\\.]*\", \"\", comment)\n",
    "    comment = re.sub(r\"(\\s\\-\\s|-$)\", \"\", comment)\n",
    "    comment = re.sub(r\"[,\\!\\?\\%\\(\\)\\/\\\"]\", \"\", comment)\n",
    "    comment = re.sub(r\"\\&\\S*\\s\", \"\", comment)\n",
    "    comment = re.sub(r\"\\&\", \"\", comment)\n",
    "    comment = re.sub(r\"\\+\", \"\", comment)\n",
    "    comment = re.sub(r\"\\#\", \"\", comment)\n",
    "    comment = re.sub(r\"\\$\", \"\", comment)\n",
    "    comment = re.sub(r\"\\¬£\", \"\", comment)\n",
    "    comment = re.sub(r\"\\%\", \"\", comment)\n",
    "    comment = re.sub(r\"\\:\", \"\", comment)\n",
    "    comment = re.sub(r\"\\@\", \"\", comment)\n",
    "    comment = re.sub(r\"\\-\", \"\", comment)\n",
    "\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(STOP_WORDS)\n",
    "deselect_stop_words = ['ne','pas','plus','personne','aucun','ni','aucune','rien']\n",
    "for w in deselect_stop_words:\n",
    "    if w in stop_words:\n",
    "        stop_words.remove(w)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying pipeline and removing stopwords from our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['text_processed'] = merged_df['text'].apply(nlp_pipeline)\n",
    "merged_df['text_processed'] = merged_df['text_processed'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the commentBlob library to get sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "\n",
    "senti_num_list = []\n",
    "senti_cat_list = []\n",
    "for i in merged_df[\"text_processed\"]:\n",
    "    vs = tb(i).sentiment[0]\n",
    "    senti_num_list.append(vs)\n",
    "    if (vs > 0.08):\n",
    "        senti_cat_list.append('Positive')\n",
    "    elif (vs < -0.08):\n",
    "        senti_cat_list.append('Negative')\n",
    "    else:\n",
    "        senti_cat_list.append('Neutral')\n",
    "\n",
    "merged_df['sentiment_num'] = senti_num_list\n",
    "merged_df['sentiment_cat'] = senti_cat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Original comment ##\n",
      "La grosse diff√©rence c'est que ceux qui emp√™chent les autres de profiter de leur dimanche... ce sont les motards justement.\n",
      "\n",
      "Perso, j'adore faire de la musique. Avec des amplis √† lampe. Fort. Et plein de distorsion. Et pas n√©cessairement des choses m√©lodieuses. J'aime la noise. Je pourrais l√©galement faire ce que je veux chez moi jusqu'√† 22h30 et faire des concertos de larsen tous les jours de la semaine si je le souhaitais.\n",
      "Sauf que...\n",
      "Je ne le fais pas. Je respecte mes voisins et leur besoin de tranquillit√©. Et j'attends d'eux qu'ils fassent de m√™me en √©change.\n",
      "\n",
      "Et quand je veux faire du bruit ? Je vais dans un lieu d√©di√© √† √ßa tout simplement (studio de r√©p√©te par exemple).\n",
      "## Score comment ##\n",
      "0.29\n"
     ]
    }
   ],
   "source": [
    "print('## Original comment ##')\n",
    "print(merged_df['text'][100])\n",
    "print('## Score comment ##')\n",
    "print(merged_df['sentiment_num'][100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved csv df in : exports/france/france_merged.csv\n",
      "Saved parquet df in : exports/france/france_merged.parquet\n"
     ]
    }
   ],
   "source": [
    "def save_transform(df:pd.DataFrame):\n",
    "    file_name = subreddit + '_merged'\n",
    "    path_csv = 'exports/france/' + file_name + '.csv'\n",
    "    df.to_csv(path_csv, index = False, encoding = 'utf-8')\n",
    "    print('Saved csv df in : ' + path_csv)\n",
    "    path_parquet = 'exports/france/' + file_name + '.parquet'\n",
    "    df.to_parquet(path_parquet, index = False, engine='pyarrow')\n",
    "    print('Saved parquet df in : ' + path_parquet)\n",
    "\n",
    "save_transform(merged_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "211f0b0304102da3630068b06045e4e4a78ffa0a6c596be87323c896afe13624"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
