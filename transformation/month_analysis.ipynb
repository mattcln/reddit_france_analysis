{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering data and getting stats on a specific month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, '../')\n",
    "from help_func import sql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = 'france'\n",
    "month = 9\n",
    "year = 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 204240 comments\n",
      "We have 11286 posts\n"
     ]
    }
   ],
   "source": [
    "comments = pd.read_parquet('exports/' + subreddit + '/' + subreddit + '_comments_merged.parquet', engine='pyarrow')\n",
    "assert len(comments[comments.duplicated(['comment_id'])]) == 0, \"Meh, I found some duplicated comments IDs in the dataframe\"\n",
    "\n",
    "posts = pd.read_parquet('exports/' + subreddit + '/' + subreddit + '_posts_merged.parquet', engine='pyarrow')\n",
    "assert len(posts[posts.duplicated(['post_id'])]) == 0, \"Meh, I found some duplicated post IDs in the dataframe\"\n",
    "\n",
    "print('We have ' + str(len(comments)) + ' comments')\n",
    "print('We have ' + str(len(posts)) + ' posts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering on comments published in the specific month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 106720 comments\n",
      "We have 6927 posts\n"
     ]
    }
   ],
   "source": [
    "comments = comments[(comments['year_comment'] == year)\n",
    "    & (comments['month_comment'] == month)]\n",
    "posts = posts[(posts['year_post'] == year)\n",
    "    & (posts['month_post'] == month)]\n",
    "print('We have ' + str(len(comments)) + ' comments')\n",
    "print('We have ' + str(len(posts)) + ' posts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating SQL databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_db = 'reddit_analysis'\n",
    "\n",
    "engine = sql.insert_df_table(f'sqlite:///{name_db}.db', 'posts', posts.reset_index(drop=True))\n",
    "engine = sql.insert_df_table(f'sqlite:///{name_db}.db', 'comments', comments.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('JeuDeLaVie',), ('Wonderful-Excuse4922',), ('Personal-Thought9453',)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connection = engine.connect()\n",
    "\n",
    "# result = connection.execute(\"SELECT * FROM posts\")\n",
    "authors_post = sql.execute_query(engine, 'SELECT author_post FROM posts')\n",
    "authors_post[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global\n",
    "### Number of posts\n",
    "### Number of comments\n",
    "### Top 3 posts with the highest number of comments (+ links)\n",
    "### Average number of comments per posts\n",
    "### Average number of words per comments\n",
    "### Number of unique authors (posts + comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_posts = posts['post_id'].nunique()\n",
    "nb_comments = comments['comment_id'].nunique()\n",
    "top_com1 = posts.nlargest(3,'nb_comment').iloc[:1]\n",
    "top_com2 = posts.nlargest(3,'nb_comment').iloc[1:2]\n",
    "top_com3 = posts.nlargest(3,'nb_comment').iloc[2:3]\n",
    "avg_comments_posts = round(posts['nb_comment'].mean(), 2)\n",
    "avg_words_comments = np.array([len(str(comment).split()) for comment in comments['text_comment']]).mean()\n",
    "nb_active_users = np.unique(comments['author_post'] + comments['author_comment']).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language\n",
    "### Top 3 words appearing the most in titles\n",
    "### Top 3 words appearing the most in comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "france    320\n",
       "pas       212\n",
       "queen     185\n",
       "«         181\n",
       "plus      179\n",
       "»         170\n",
       "new       133\n",
       "|         130\n",
       "ne        127\n",
       "best      121\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_titles_words = pd.Series(' '.join(posts['title_processed']).split()).value_counts()[:10]\n",
    "top_titles_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pas      70674\n",
       "c'est    48364\n",
       "ne       28210\n",
       "plus     26680\n",
       "bien     13778\n",
       "faire    13639\n",
       "j'ai     12402\n",
       "qu'il     8162\n",
       "non       7793\n",
       "c’est     6855\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_comments_words = pd.Series(' '.join(comments['text_processed']).split()).value_counts()[:10]\n",
    "top_comments_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flairs\n",
    "### Number of posts per flair\n",
    "### Posts with the highest number of comments per flair (linked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_posts_flairs = posts.groupby(['flair']).size().sort_values(ascending=False)[:3].reset_index()\n",
    "biggest_post_flair1 = posts[posts['flair'] == nb_posts_flairs['flair'][0]].nlargest(1,'nb_comment')\n",
    "biggest_post_flair2 = posts[posts['flair'] == nb_posts_flairs['flair'][1]].nlargest(1,'nb_comment')\n",
    "biggest_post_flair3 = posts[posts['flair'] == nb_posts_flairs['flair'][2]].nlargest(1,'nb_comment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users average\n",
    "### Average posts per actif users\n",
    "### Average comments per users\n",
    "### Average number of words per users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_posts_user = posts.groupby(['author_post']).size().sum()/nb_active_users\n",
    "avg_comments_user = comments.groupby(['author_comment']).size().mean()\n",
    "avg_words_user = comments.groupby(['author_comment'])['nb_words_comment'].mean().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users records\n",
    "### Users with the highest number of posts\n",
    "### Users with the highest number of comments\n",
    "### Users with the highest number of words\n",
    "### User with the best vocabulary (highest number of unique words)\n",
    "### User that removed the most of his posts\n",
    "### User that wrote the longest comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('latestasianews', 2340),\n",
       " ('RIFTV_news', 220),\n",
       " ('Hellvis_50s', 57),\n",
       " ('Fearless-Cricket3297', 53),\n",
       " ('FrankMaleir', 50)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_nb_posts_user_query = \"\"\"\n",
    "SELECT author_post,\n",
    "nb_posts\n",
    "FROM\n",
    "(SELECT author_post,\n",
    "COUNT(DISTINCT post_id) as nb_posts\n",
    "FROM posts\n",
    "GROUP BY author_post)\n",
    "ORDER BY nb_posts DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "highest_nb_posts_user = sql.execute_query(engine, highest_nb_posts_user_query)\n",
    "highest_nb_posts_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('morinl', 1033),\n",
       " ('AutoModerator', 713),\n",
       " ('anyatrans', 484),\n",
       " ('Elegant-Variety-7482', 427),\n",
       " ('Irkam', 410)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_nb_comments_user_query = \"\"\"\n",
    "SELECT author_comment,\n",
    "nb_comments\n",
    "FROM\n",
    "(SELECT author_comment,\n",
    "COUNT(DISTINCT comment_id) as nb_comments\n",
    "FROM comments\n",
    "WHERE author_comment != '[deleted]'\n",
    "GROUP BY author_comment)\n",
    "ORDER BY nb_comments DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "highest_nb_comments_user = sql.execute_query(engine, highest_nb_comments_user_query)\n",
    "highest_nb_comments_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_nb_words_user = comments.groupby(['author_comment'])['nb_words_comment'].sum().reset_index().sort_values('nb_words_comment', ascending=False).reset_index(drop=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['post_id', 'title', 'text_post', 'url', 'author_post', 'permalink_post',\n",
       "       'flair', 'author_comment', 'comment_id', 'text_comment',\n",
       "       'parent_comment_id', 'permalink_comment', 'year_comment',\n",
       "       'month_comment', 'day_comment', 'year_post', 'month_post', 'day_post',\n",
       "       'text_processed', 'title_processed', 'sentiment_num', 'sentiment_cat',\n",
       "       'nb_words_post', 'nb_words_comment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_unique_words(r) -> int:\n",
    "    \"\"\"\n",
    "    Count unique number of words in a string of a row\n",
    "    \"\"\"\n",
    "    comment = r.text_comment.lower()\n",
    "    comment = comment.replace(\".\",\" \")\n",
    "    comment = comment.replace(\",\",\" \")\n",
    "    comment = comment.replace(\":\",\" \")\n",
    "    comment = comment.replace(\";\",\" \")\n",
    "    comment = comment.replace(\"?\",\" \")\n",
    "    comment = comment.replace(r'\\s+|\\\\n', ' ') \n",
    "    words = comment.split(\" \")\n",
    "    unique_words = []\n",
    "    nb_unique_words = 0\n",
    "    for word in words:\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "            nb_unique_words += 1\n",
    "    return nb_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_comment</th>\n",
       "      <th>text_comment</th>\n",
       "      <th>nb_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>Bandolinho2</td>\n",
       "      <td>C'est même un peu la marque de fabrique de l'é...</td>\n",
       "      <td>12478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_comment                                       text_comment  \\\n",
       "880    Bandolinho2  C'est même un peu la marque de fabrique de l'é...   \n",
       "\n",
       "     nb_unique_words  \n",
       "880            12478  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_concat_df = comments[comments['text_comment'].str.len() > 0].groupby(['author_comment'], as_index=False).agg({'text_comment': ' '.join})\n",
    "comments_concat_df['nb_unique_words'] = comments_concat_df.apply(nb_unique_words, axis = 1)\n",
    "highest_vocabulary_user = comments_concat_df.sort_values('nb_unique_words', ascending=False)[:1]\n",
    "highest_vocabulary_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['post_id', 'title', 'text_post', 'url', 'author_post', 'permalink_post',\n",
       "       'flair', 'author_comment', 'comment_id', 'text_comment',\n",
       "       'parent_comment_id', 'permalink_comment', 'year_comment',\n",
       "       'month_comment', 'day_comment', 'year_post', 'month_post', 'day_post',\n",
       "       'text_processed', 'title_processed', 'sentiment_num', 'sentiment_cat',\n",
       "       'nb_words_post', 'nb_words_comment', 'len_comments', 'len_comment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_comment</th>\n",
       "      <th>len_comment</th>\n",
       "      <th>permalink_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45372</th>\n",
       "      <td>GreyArrowMonkey</td>\n",
       "      <td>9973.0</td>\n",
       "      <td>/r/france/comments/xdhbme/meilleur_combo_banqu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        author_comment  len_comment  \\\n",
       "45372  GreyArrowMonkey       9973.0   \n",
       "\n",
       "                                       permalink_comment  \n",
       "45372  /r/france/comments/xdhbme/meilleur_combo_banqu...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments['len_comment'] = comments['text_comment'].str.len()\n",
    "longest_comment_user = comments[['author_comment', 'len_comment','permalink_comment']].sort_values('len_comment', ascending=False)[:1]\n",
    "longest_comment_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/r/france/comments/xdhbme/meilleur_combo_banques_best_free_bank_combo/ioaxarb/']\n"
     ]
    }
   ],
   "source": [
    "print(longest_comment_user['permalink_comment'].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('reddit_analysis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "211f0b0304102da3630068b06045e4e4a78ffa0a6c596be87323c896afe13624"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
