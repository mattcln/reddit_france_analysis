{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering data and getting stats on a specific month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, '../')\n",
    "from help_func import sql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = 'france'\n",
    "month = 11\n",
    "year = 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 359835 comments\n",
      "We have 16102 posts\n"
     ]
    }
   ],
   "source": [
    "comments = pd.read_parquet('exports/' + subreddit + '/' + subreddit + '_comments_merged.parquet', engine='pyarrow')\n",
    "assert len(comments[comments.duplicated(['comment_id'])]) == 0, \"Meh, I found some duplicated comments IDs in the dataframe\"\n",
    "\n",
    "posts = pd.read_parquet('exports/' + subreddit + '/' + subreddit + '_posts_merged.parquet', engine='pyarrow')\n",
    "assert len(posts[posts.duplicated(['post_id'])]) == 0, \"Meh, I found some duplicated post IDs in the dataframe\"\n",
    "\n",
    "print('We have ' + str(len(comments)) + ' comments')\n",
    "print('We have ' + str(len(posts)) + ' posts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering on comments published in the specific month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 93932 comments\n",
      "We have 4515 posts\n"
     ]
    }
   ],
   "source": [
    "comments = comments[(comments['year_comment'] == year)\n",
    "    & (comments['month_comment'] == month)]\n",
    "posts = posts[(posts['year_post'] == year)\n",
    "    & (posts['month_post'] == month)]\n",
    "print('We have ' + str(len(comments)) + ' comments')\n",
    "print('We have ' + str(len(posts)) + ' posts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating SQL databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_db = 'reddit_analysis'\n",
    "\n",
    "engine = sql.insert_df_table(f'sqlite:///{name_db}.db', 'posts', posts.reset_index(drop=True))\n",
    "engine = sql.insert_df_table(f'sqlite:///{name_db}.db', 'comments', comments.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('JeuDeLaVie',), ('Wonderful-Excuse4922',), ('Personal-Thought9453',)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connection = engine.connect()\n",
    "\n",
    "# result = connection.execute(\"SELECT * FROM posts\")\n",
    "authors_post = sql.execute_query(engine, 'SELECT author_post FROM posts')\n",
    "authors_post[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global\n",
    "### Number of posts\n",
    "### Number of comments\n",
    "### Top 3 posts with the highest number of comments (+ links)\n",
    "### Average number of comments per posts\n",
    "### Average number of words per comments\n",
    "### Number of unique authors (posts + comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_posts = posts['post_id'].nunique()\n",
    "nb_comments = comments['comment_id'].nunique()\n",
    "top_com1 = posts.nlargest(3,'nb_comment').iloc[:1]\n",
    "top_com2 = posts.nlargest(3,'nb_comment').iloc[1:2]\n",
    "top_com3 = posts.nlargest(3,'nb_comment').iloc[2:3]\n",
    "avg_comments_posts = round(posts['nb_comment'].mean(), 2)\n",
    "avg_words_comments = np.array([len(str(comment).split()) for comment in comments['text_comment']]).mean()\n",
    "nb_active_users = np.unique(comments['author_post'] + comments['author_comment']).size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language\n",
    "### Top 3 words appearing the most in titles\n",
    "### Top 3 words appearing the most in comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "france      372\n",
       "pas         214\n",
       "«           166\n",
       "»           165\n",
       "plus        139\n",
       "monde       134\n",
       "français    134\n",
       "ne          132\n",
       "french      109\n",
       "paris       107\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_titles_words = pd.Series(' '.join(posts['title_processed']).split()).value_counts()[:10]\n",
    "top_titles_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pas      60994\n",
       "c'est    41064\n",
       "ne       25094\n",
       "plus     22898\n",
       "bien     12163\n",
       "faire    11441\n",
       "j'ai     10647\n",
       "qu'il     6931\n",
       "non       6583\n",
       "c’est     5924\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_comments_words = pd.Series(' '.join(comments['text_processed']).split()).value_counts()[:10]\n",
    "top_comments_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flairs\n",
    "### Number of posts per flair\n",
    "### Posts with the highest number of comments per flair (linked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_posts_flairs = posts.groupby(['flair']).size().sort_values(ascending=False)[:3].reset_index()\n",
    "biggest_post_flair1 = posts[posts['flair'] == nb_posts_flairs['flair'][0]].nlargest(1,'nb_comment')\n",
    "biggest_post_flair2 = posts[posts['flair'] == nb_posts_flairs['flair'][1]].nlargest(1,'nb_comment')\n",
    "biggest_post_flair3 = posts[posts['flair'] == nb_posts_flairs['flair'][2]].nlargest(1,'nb_comment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users average\n",
    "### Average posts per actif users\n",
    "### Average comments per users\n",
    "### Average number of words per users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_posts_user = posts.groupby(['author_post']).size().sum()/nb_active_users\n",
    "avg_comments_user = comments.groupby(['author_comment']).size().mean()\n",
    "avg_words_user = comments.groupby(['author_comment'])['nb_words_comment'].mean().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users records\n",
    "### Users with the highest number of posts\n",
    "### Users with the highest number of comments\n",
    "### Users with the highest number of words\n",
    "### User with the best vocabulary (highest number of unique words)\n",
    "### User that removed the most of his posts\n",
    "### User that wrote the longest comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('latestasianews', 2340),\n",
       " ('RIFTV_news', 220),\n",
       " ('FrankMaleir', 158),\n",
       " ('Fearless-Cricket3297', 138),\n",
       " ('GrenobleLyon', 131)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_nb_posts_user_query = \"\"\"\n",
    "SELECT author_post,\n",
    "nb_posts\n",
    "FROM\n",
    "(SELECT author_post,\n",
    "COUNT(DISTINCT post_id) as nb_posts\n",
    "FROM posts\n",
    "GROUP BY author_post)\n",
    "ORDER BY nb_posts DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "highest_nb_posts_user = sql.execute_query(engine, highest_nb_posts_user_query)\n",
    "highest_nb_posts_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('morinl', 3399),\n",
       " ('AutoModerator', 1957),\n",
       " ('anyatrans', 1446),\n",
       " ('AzuNetia', 1180),\n",
       " ('Elegant-Variety-7482', 1147)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_nb_comments_user_query = \"\"\"\n",
    "SELECT author_comment,\n",
    "nb_comments\n",
    "FROM\n",
    "(SELECT author_comment,\n",
    "COUNT(DISTINCT comment_id) as nb_comments\n",
    "FROM comments\n",
    "WHERE author_comment != '[deleted]'\n",
    "GROUP BY author_comment)\n",
    "ORDER BY nb_comments DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "highest_nb_comments_user = sql.execute_query(engine, highest_nb_comments_user_query)\n",
    "highest_nb_comments_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_nb_words_user = comments.groupby(['author_comment'])['nb_words_comment'].sum().reset_index().sort_values('nb_words_comment', ascending=False).reset_index(drop=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_unique_words(r) -> int:\n",
    "    \"\"\"\n",
    "    Count unique number of words in a string of a row\n",
    "    \"\"\"\n",
    "    comment = r.text_comment.lower()\n",
    "    comment = comment.replace(\".\",\" \")\n",
    "    comment = comment.replace(\",\",\" \")\n",
    "    comment = comment.replace(\":\",\" \")\n",
    "    comment = comment.replace(\";\",\" \")\n",
    "    comment = comment.replace(\"?\",\" \")\n",
    "    comment = comment.replace(r'\\s+|\\\\n', ' ') \n",
    "    words = comment.split(\" \")\n",
    "    unique_words = []\n",
    "    nb_unique_words = 0\n",
    "    for word in words:\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "            nb_unique_words += 1\n",
    "    return nb_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_comment</th>\n",
       "      <th>text_comment</th>\n",
       "      <th>nb_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2633</th>\n",
       "      <td>Folivao</td>\n",
       "      <td>Abandon de poste = faute grave : tu touches pa...</td>\n",
       "      <td>9470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     author_comment                                       text_comment  \\\n",
       "2633        Folivao  Abandon de poste = faute grave : tu touches pa...   \n",
       "\n",
       "      nb_unique_words  \n",
       "2633             9470  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_concat_df = comments[comments['text_comment'].str.len() > 0].groupby(['author_comment'], as_index=False).agg({'text_comment': ' '.join})\n",
    "comments_concat_df['nb_unique_words'] = comments_concat_df.apply(nb_unique_words, axis = 1)\n",
    "highest_vocabulary_user = comments_concat_df.sort_values('nb_unique_words', ascending=False)[:1]\n",
    "highest_vocabulary_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_comment</th>\n",
       "      <th>len_comment</th>\n",
       "      <th>permalink_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>306151</th>\n",
       "      <td>Folivao</td>\n",
       "      <td>9997.0</td>\n",
       "      <td>/r/france/comments/yusxfg/la_juge_belge_qui_a_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       author_comment  len_comment  \\\n",
       "306151        Folivao       9997.0   \n",
       "\n",
       "                                        permalink_comment  \n",
       "306151  /r/france/comments/yusxfg/la_juge_belge_qui_a_...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments['len_comment'] = comments['text_comment'].str.len()\n",
    "longest_comment_user = comments[['author_comment', 'len_comment','permalink_comment']].sort_values('len_comment', ascending=False)[:1]\n",
    "longest_comment_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/r/france/comments/yusxfg/la_juge_belge_qui_a_arrêté_les_terroristes_du/iwb1o58/']\n"
     ]
    }
   ],
   "source": [
    "print(longest_comment_user['permalink_comment'].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('reddit_analysis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "211f0b0304102da3630068b06045e4e4a78ffa0a6c596be87323c896afe13624"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
