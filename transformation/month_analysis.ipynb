{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering data and getting stats on a specific month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(0, '../')\n",
    "from help_func import sql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = 'france'\n",
    "month = 9\n",
    "year = 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 359835 comments\n",
      "We have 16102 posts\n"
     ]
    }
   ],
   "source": [
    "comments = pd.read_parquet('exports/' + subreddit + '/' + subreddit + '_comments_merged.parquet', engine='pyarrow')\n",
    "assert len(comments[comments.duplicated(['comment_id'])]) == 0, \"Meh, I found some duplicated comments IDs in the dataframe\"\n",
    "\n",
    "posts = pd.read_parquet('exports/' + subreddit + '/' + subreddit + '_posts_merged.parquet', engine='pyarrow')\n",
    "assert len(posts[posts.duplicated(['post_id'])]) == 0, \"Meh, I found some duplicated post IDs in the dataframe\"\n",
    "\n",
    "print('We have ' + str(len(comments)) + ' comments')\n",
    "print('We have ' + str(len(posts)) + ' posts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering on comments published in the specific month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 137539 comments\n",
      "We have 6927 posts\n"
     ]
    }
   ],
   "source": [
    "comments = comments[(comments['year_comment'] == year)\n",
    "    & (comments['month_comment'] == month)]\n",
    "posts = posts[(posts['year_post'] == year)\n",
    "    & (posts['month_post'] == month)]\n",
    "print('We have ' + str(len(comments)) + ' comments')\n",
    "print('We have ' + str(len(posts)) + ' posts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating SQL databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_db = 'reddit_analysis'\n",
    "\n",
    "engine = sql.insert_df_table(f'sqlite:///{name_db}.db', 'posts', posts.reset_index(drop=True))\n",
    "engine = sql.insert_df_table(f'sqlite:///{name_db}.db', 'comments', comments.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('JeuDeLaVie',), ('Wonderful-Excuse4922',), ('Personal-Thought9453',)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connection = engine.connect()\n",
    "\n",
    "# result = connection.execute(\"SELECT * FROM posts\")\n",
    "authors_post = sql.execute_query(engine, 'SELECT author_post FROM posts')\n",
    "authors_post[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global\n",
    "### Number of posts\n",
    "### Number of comments\n",
    "### Top 3 posts with the highest number of comments (+ links)\n",
    "### Average number of comments per posts\n",
    "### Average number of words per comments\n",
    "### Number of unique authors (posts + comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts : 6927\n",
      "Number of comments : 137539\n",
      "Top 1 avec le plus de commentaires titre : [\"Quel fut votre pire entretien d'embauche ? (ou à défaut, le plus étrange ?)\"]\n",
      "Top 1 avec le plus de commentaires titre : [576]\n",
      "Top 1 avec le plus de commentaires url : ['/r/france/comments/x87qgc/quel_fut_votre_pire_entretien_dembauche_ou_à/']\n",
      "Top 2 avec le plus de commentaires titre : ['Islamisme à l\\'école : \"Deux femmes voilées prises à partie les émeuvent plus que Samuel Paty\"']\n",
      "Top 1 avec le plus de commentaires titre : [496]\n",
      "Top 2 avec le plus de commentaires url : ['/r/france/comments/x3lrpv/islamisme_à_lécole_deux_femmes_voilées_prises_à/']\n",
      "Top 3 avec le plus de commentaires titre : [\"Rendre attractif le métier d'enseignant: raté!\"]\n",
      "Top 1 avec le plus de commentaires titre : [496]\n",
      "Top 3 avec le plus de commentaires url : ['/r/france/comments/x9ptf9/rendre_attractif_le_métier_denseignant_raté/']\n",
      "Nombre de commentaires moyen par postes : 20.03\n",
      "Nombre de mots moyen par commentaire : 41.40417627000342\n",
      "Nombre d\"utilisateurs actifs 75667\n"
     ]
    }
   ],
   "source": [
    "nb_posts = posts['post_id'].nunique()\n",
    "nb_comments = comments['comment_id'].nunique()\n",
    "top_com1 = posts.nlargest(3,'nb_comment').iloc[:1]\n",
    "top_com2 = posts.nlargest(3,'nb_comment').iloc[1:2]\n",
    "top_com3 = posts.nlargest(3,'nb_comment').iloc[2:3]\n",
    "avg_comments_posts = round(posts['nb_comment'].mean(), 2)\n",
    "avg_words_comments = np.array([len(str(comment).split()) for comment in comments['text_comment']]).mean()\n",
    "nb_active_users = np.unique(comments['author_post'] + comments['author_comment']).size\n",
    "\n",
    "### print\n",
    "print(f'Number of posts : {nb_posts}')\n",
    "print(f'Number of comments : {nb_comments}')\n",
    "print(f'Top 1 avec le plus de commentaires titre : {top_com1.title.values}')\n",
    "print(f'Top 1 avec le plus de commentaires titre : {top_com1.nb_comment.values}')\n",
    "print(f'Top 1 avec le plus de commentaires url : {top_com1.permalink_post.values}')\n",
    "print(f'Top 2 avec le plus de commentaires titre : {top_com2.title.values}')\n",
    "print(f'Top 1 avec le plus de commentaires titre : {top_com2.nb_comment.values}')\n",
    "print(f'Top 2 avec le plus de commentaires url : {top_com2.permalink_post.values}')\n",
    "print(f'Top 3 avec le plus de commentaires titre : {top_com3.title.values}')\n",
    "print(f'Top 1 avec le plus de commentaires titre : {top_com3.nb_comment.values}')\n",
    "print(f'Top 3 avec le plus de commentaires url : {top_com3.permalink_post.values}')\n",
    "print(f'Nombre de commentaires moyen par postes : {avg_comments_posts}')\n",
    "print(f'Nombre de mots moyen par commentaire : {avg_words_comments}')\n",
    "print(f'Nombre d\"utilisateurs actifs {nb_active_users}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language\n",
    "### Top 3 words appearing the most in titles\n",
    "### Top 3 words appearing the most in comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "france    320\n",
       "pas       212\n",
       "queen     185\n",
       "«         181\n",
       "plus      179\n",
       "»         170\n",
       "new       133\n",
       "|         130\n",
       "ne        127\n",
       "best      121\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_titles_words = pd.Series(' '.join(posts['title_processed']).split()).value_counts()[:10]\n",
    "top_titles_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pas      92216\n",
       "c'est    63397\n",
       "ne       36493\n",
       "plus     34551\n",
       "bien     18085\n",
       "faire    17960\n",
       "j'ai     16537\n",
       "qu'il    10735\n",
       "non      10118\n",
       "c’est     8962\n",
       "faut      8533\n",
       "rien      7636\n",
       "qu'on     7586\n",
       "oui       7562\n",
       "n'est     7341\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_comments_words = pd.Series(' '.join(comments['text_processed']).split()).value_counts()[:15]\n",
    "top_comments_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flairs\n",
    "### Number of posts per flair\n",
    "### Posts with the highest number of comments per flair (linked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_posts_flairs = posts.groupby(['flair']).size().sort_values(ascending=False)[:3].reset_index()\n",
    "biggest_post_flair1 = posts[posts['flair'] == nb_posts_flairs['flair'][0]].nlargest(1,'nb_comment')\n",
    "biggest_post_flair2 = posts[posts['flair'] == nb_posts_flairs['flair'][1]].nlargest(1,'nb_comment')\n",
    "biggest_post_flair3 = posts[posts['flair'] == nb_posts_flairs['flair'][2]].nlargest(1,'nb_comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flair</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ask France</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Société</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Politique</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         flair    0\n",
       "0  Ask France   461\n",
       "1      Société  328\n",
       "2    Politique  287"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_posts_flairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flair</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ask France</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Société</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Politique</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Actus</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Culture</td>\n",
       "      <td>193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Forum Libre</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Écologie</td>\n",
       "      <td>162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Paywall</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Humour</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Économie</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Méta</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Science</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sport</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Musique</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Fait Divers</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Covid-19</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Soft paywall</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Annonce</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Cinéma Séries</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Trompeur ?</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AMA</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Elections</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            flair    0\n",
       "0     Ask France   461\n",
       "1         Société  328\n",
       "2       Politique  287\n",
       "3           Actus  248\n",
       "4         Culture  193\n",
       "5     Forum Libre  187\n",
       "6        Écologie  162\n",
       "7         Paywall  139\n",
       "8          Humour  129\n",
       "9        Économie  126\n",
       "10           Méta   75\n",
       "11        Science   56\n",
       "12          Sport   51\n",
       "13        Musique   39\n",
       "14    Fait Divers   39\n",
       "15       Covid-19   28\n",
       "16   Soft paywall   27\n",
       "17        Annonce   24\n",
       "18  Cinéma Séries   19\n",
       "19     Trompeur ?   14\n",
       "20            AMA   12\n",
       "21      Elections    2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts.groupby(['flair']).size().sort_values(ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poste avec le plus de commentaires du flair top 1 : ['C\\'est quoi le problème des Français avec le \"manque d\\'autorité\" ?']\n",
      "Poste avec le plus de commentaires du flair top 1 : [298]\n",
      "Poste avec le plus de commentaires du flair top 1 : ['/r/france/comments/xdvue1/cest_quoi_le_problème_des_français_avec_le_manque/']\n",
      "Poste avec le plus de commentaires du flair top 2 : ['Islamisme à l\\'école : \"Deux femmes voilées prises à partie les émeuvent plus que Samuel Paty\"']\n",
      "Poste avec le plus de commentaires du flair top 2 : [496]\n",
      "Poste avec le plus de commentaires du flair top 2 : ['/r/france/comments/x3lrpv/islamisme_à_lécole_deux_femmes_voilées_prises_à/']\n",
      "Poste avec le plus de commentaires du flair top 3 : ['Gauche du travail contre gauche des allocs : «On a un droit à la paresse», estime Sandrine Rousseau']\n",
      "Poste avec le plus de commentaires du flair top 3 : [427]\n",
      "Poste avec le plus de commentaires du flair top 3 : ['/r/france/comments/xes40b/gauche_du_travail_contre_gauche_des_allocs_on_a/']\n"
     ]
    }
   ],
   "source": [
    "print(f'Poste avec le plus de commentaires du flair top 1 : {biggest_post_flair1.title.values}')\n",
    "print(f'Poste avec le plus de commentaires du flair top 1 : {biggest_post_flair1.nb_comment.values}')\n",
    "print(f'Poste avec le plus de commentaires du flair top 1 : {biggest_post_flair1.permalink_post.values}')\n",
    "print(f'Poste avec le plus de commentaires du flair top 2 : {biggest_post_flair2.title.values}')\n",
    "print(f'Poste avec le plus de commentaires du flair top 2 : {biggest_post_flair2.nb_comment.values}')\n",
    "print(f'Poste avec le plus de commentaires du flair top 2 : {biggest_post_flair2.permalink_post.values}')\n",
    "print(f'Poste avec le plus de commentaires du flair top 3 : {biggest_post_flair3.title.values}')\n",
    "print(f'Poste avec le plus de commentaires du flair top 3 : {biggest_post_flair3.nb_comment.values}')\n",
    "print(f'Poste avec le plus de commentaires du flair top 3 : {biggest_post_flair3.permalink_post.values}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users average\n",
    "### Average posts per actif users\n",
    "### Average comments per users\n",
    "### Average number of words per users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de postes moyen par utilisateur : 0.09154585222091532\n",
      "Nombre de commentaires moyen par utilisateur : 9.502487218460688\n",
      "Nombre de mots moyen par utilisateur : 38.73925817759708\n"
     ]
    }
   ],
   "source": [
    "avg_posts_user = posts.groupby(['author_post']).size().sum()/nb_active_users\n",
    "avg_comments_user = comments.groupby(['author_comment']).size().mean()\n",
    "avg_words_user = comments.groupby(['author_comment'])['nb_words_comment'].mean().mean()\n",
    "print(f'Nombre de postes moyen par utilisateur : {avg_posts_user}')\n",
    "print(f'Nombre de commentaires moyen par utilisateur : {avg_comments_user}')\n",
    "print(f'Nombre de mots moyen par utilisateur : {avg_words_user}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users records\n",
    "### Users with the highest number of posts\n",
    "### Users with the highest number of comments\n",
    "### Users with the highest number of words\n",
    "### User with the best vocabulary (highest number of unique words)\n",
    "### User that removed the most of his posts\n",
    "### User that wrote the longest comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('latestasianews', 2340),\n",
       " ('RIFTV_news', 220),\n",
       " ('Hellvis_50s', 57),\n",
       " ('Fearless-Cricket3297', 53),\n",
       " ('FrankMaleir', 50)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_nb_posts_user_query = \"\"\"\n",
    "SELECT author_post,\n",
    "nb_posts\n",
    "FROM\n",
    "(SELECT author_post,\n",
    "COUNT(DISTINCT post_id) as nb_posts\n",
    "FROM posts\n",
    "GROUP BY author_post)\n",
    "ORDER BY nb_posts DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "highest_nb_posts_user = sql.execute_query(engine, highest_nb_posts_user_query)\n",
    "highest_nb_posts_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('morinl', 1367),\n",
       " ('AutoModerator', 891),\n",
       " ('Irkam', 543),\n",
       " ('anyatrans', 530),\n",
       " ('Elegant-Variety-7482', 493)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_nb_comments_user_query = \"\"\"\n",
    "SELECT author_comment,\n",
    "nb_comments\n",
    "FROM\n",
    "(SELECT author_comment,\n",
    "COUNT(DISTINCT comment_id) as nb_comments\n",
    "FROM comments\n",
    "WHERE author_comment != '[deleted]'\n",
    "GROUP BY author_comment)\n",
    "ORDER BY nb_comments DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "highest_nb_comments_user = sql.execute_query(engine, highest_nb_comments_user_query)\n",
    "highest_nb_comments_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_comment</th>\n",
       "      <th>nb_words_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bandolinho2</td>\n",
       "      <td>92009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>69895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>morinl</td>\n",
       "      <td>43305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IntelArtiGen</td>\n",
       "      <td>36165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NoFrontiers</td>\n",
       "      <td>24138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author_comment  nb_words_comment\n",
       "0    Bandolinho2             92009\n",
       "1  AutoModerator             69895\n",
       "2         morinl             43305\n",
       "3   IntelArtiGen             36165\n",
       "4    NoFrontiers             24138"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_nb_words_user = comments.groupby(['author_comment'])['nb_words_comment'].sum().reset_index().sort_values('nb_words_comment', ascending=False).reset_index(drop=True)[:5]\n",
    "highest_nb_words_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_unique_words(r) -> int:\n",
    "    \"\"\"\n",
    "    Count unique number of words in a string of a row\n",
    "    \"\"\"\n",
    "    comment = r.text_comment.lower()\n",
    "    comment = comment.replace(\".\",\" \")\n",
    "    comment = comment.replace(\",\",\" \")\n",
    "    comment = comment.replace(\":\",\" \")\n",
    "    comment = comment.replace(\";\",\" \")\n",
    "    comment = comment.replace(\"?\",\" \")\n",
    "    comment = comment.replace(r'\\s+|\\\\n', ' ') \n",
    "    words = comment.split(\" \")\n",
    "    unique_words = []\n",
    "    nb_unique_words = 0\n",
    "    for word in words:\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "            nb_unique_words += 1\n",
    "    return nb_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_comment</th>\n",
       "      <th>text_comment</th>\n",
       "      <th>nb_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>Bandolinho2</td>\n",
       "      <td>Je suis quand même assez étonné (mais pas tant...</td>\n",
       "      <td>14605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_comment                                       text_comment  \\\n",
       "979    Bandolinho2  Je suis quand même assez étonné (mais pas tant...   \n",
       "\n",
       "     nb_unique_words  \n",
       "979            14605  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_concat_df = comments[comments['text_comment'].str.len() > 0].groupby(['author_comment'], as_index=False).agg({'text_comment': ' '.join})\n",
    "comments_concat_df['nb_unique_words'] = comments_concat_df.apply(nb_unique_words, axis = 1)\n",
    "highest_vocabulary_user = comments_concat_df.sort_values('nb_unique_words', ascending=False)[:1]\n",
    "highest_vocabulary_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_comment</th>\n",
       "      <th>len_comment</th>\n",
       "      <th>permalink_comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58906</th>\n",
       "      <td>GreyArrowMonkey</td>\n",
       "      <td>9973.0</td>\n",
       "      <td>/r/france/comments/xdhbme/meilleur_combo_banqu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        author_comment  len_comment  \\\n",
       "58906  GreyArrowMonkey       9973.0   \n",
       "\n",
       "                                       permalink_comment  \n",
       "58906  /r/france/comments/xdhbme/meilleur_combo_banqu...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments['len_comment'] = comments['text_comment'].str.len()\n",
    "longest_comment_user = comments[['author_comment', 'len_comment','permalink_comment']].sort_values('len_comment', ascending=False)[:1]\n",
    "longest_comment_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/r/france/comments/xdhbme/meilleur_combo_banques_best_free_bank_combo/ioaxarb/']\n"
     ]
    }
   ],
   "source": [
    "print(longest_comment_user['permalink_comment'].values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('reddit_analysis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "211f0b0304102da3630068b06045e4e4a78ffa0a6c596be87323c896afe13624"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
