{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are your parameters ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get('https://api.fr')\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pushshift_data(after, before, subreddit) -> dict():\n",
    "    try:\n",
    "        URL = 'https://api.pushshift.io/reddit/submission/search/?subreddit='+str(subreddit)+'&after='+str(after)+'&before='+str(before)\n",
    "        print(URL)\n",
    "        r = requests.get(URL)\n",
    "        if r.status_code == 200:\n",
    "            data = json.loads(r.text, strict = False)\n",
    "            return data['data']\n",
    "        \n",
    "        #Si on a eu une erreur en récupérant l'URL on réessaye 5 fois, sinon on abandonne\n",
    "        else:\n",
    "            nb_try = 0\n",
    "            while r.status_code != 200 | nb_try < 5:\n",
    "                URL = 'https://api.pushshift.io/reddit/submission/search/?subreddit='+str(subreddit)+'&after='+str(after)+'&before='+str(before)\n",
    "                print(URL)\n",
    "                r = requests.get(URL)\n",
    "                data = json.loads(r.text, strict = False)\n",
    "                nb_try += 1\n",
    "            if r.status_code == 200:           \n",
    "                return data['data']\n",
    "            else: return ''\n",
    "    except:\n",
    "        print('Error while accessing API')\n",
    "        print(r)\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collect_clean_data(subpost, columns) -> pd.Series():\n",
    "    clean_data = list()\n",
    "    title = subpost['title']\n",
    "    url = subpost['url']\n",
    "    try:\n",
    "        flair = subpost['link_flair_text']\n",
    "    except KeyError:\n",
    "        flair = 'NaN'\n",
    "    try:\n",
    "        body = subpost['selftext']\n",
    "    except KeyError:\n",
    "        body = ''\n",
    "    author = subpost['author']\n",
    "    postId = subpost['id']\n",
    "    # score = subpost['score']\n",
    "    created = datetime.fromtimestamp(subpost['created_utc'])\n",
    "    # num_com = subpost['num_comments']\n",
    "    permalink = subpost['permalink']    \n",
    "    return pd.Series([postId,title,body,url,author,created,permalink,flair], index = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def loop_between_dates(after, before, subreddit) -> pd.DataFrame():\n",
    "    columns = ['postId','title','body','url','author','created','permalink','flair']\n",
    "    rows_list = []\n",
    "\n",
    "    #First call of the API with the original after / before intervals and the subreddit\n",
    "    data = get_pushshift_data(after, before, subreddit)\n",
    "\n",
    "    #While our API calls are returning something, we keep scrapping\n",
    "    while len(data) > 0:\n",
    "        for subpost in data:\n",
    "            subpost_list = collect_clean_data(subpost, columns)\n",
    "            rows_list.append(subpost_list)\n",
    "\n",
    "        #Printing number of posts returned by the API (max = 25)\n",
    "        print(len(data))\n",
    "\n",
    "        #Getting the timestamp of the last post scrapped and replacing the old 'after' value\n",
    "        print(str(datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "        after = data[-1]['created_utc']\n",
    "\n",
    "        #New request with the new 'after' interval\n",
    "        data = get_pushshift_data(after, before, subreddit)\n",
    "\n",
    "    return pd.DataFrame(rows_list, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1648714089.007502"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1648714089.175328\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ts = datetime.now().timestamp()\n",
    "print(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1647298800.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = datetime.timestamp(datetime(2022, 3, 15))\n",
    "before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2022, 3, 15)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = datetime.fromtimestamp(before)\n",
    "before.date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20220315'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = str(before.strftime(\"%Y\")) +  str(before.strftime(\"%m\")) + str(before.strftime(\"%d\"))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2022"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = datetime.timestamp(datetime(2022, 3, 15))\n",
    "int(datetime.fromtimestamp(before).strftime(\"%Y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after = datetime.timestamp(datetime(2021, 12, 31))\n",
    "int(datetime.fromtimestamp(after).strftime(\"%Y\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nb_months(after, before) -> int:\n",
    "    return (int(datetime.fromtimestamp(before).strftime(\"%Y\")) - int(datetime.fromtimestamp(after).strftime(\"%Y\"))) * 12 + int(datetime.fromtimestamp(before).strftime(\"%m\")) - int(datetime.fromtimestamp(after).strftime(\"%m\"))\n",
    "\n",
    "def add_one_month(date_timestamp) -> str:\n",
    "    months_31 = ['01', '03', '05','07','08','10','12']\n",
    "    months_30 = ['04', '06', '09', '11']\n",
    "    if datetime.fromtimestamp(ts).strftime(\"%m\") in months_31:\n",
    "        return date_timestamp + 86400*31\n",
    "    elif datetime.fromtimestamp(ts).strftime(\"%m\") in months_30:\n",
    "        return date_timestamp + 86400 * 30\n",
    "    else: return date_timestamp + 86400 * 28\n",
    "\n",
    "def save_dataframe(df, after, before):\n",
    "    #On veut retransformer les timestamps en date\n",
    "    after = str(after.strftime(\"%Y\")) +  str(after.strftime(\"%m\")) + str(after.strftime(\"%d\"))\n",
    "    before = str(before.strftime(\"%Y\")) +  str(before.strftime(\"%m\")) + str(before.strftime(\"%d\"))\n",
    "    csv_file_name = 'france_' + str(after) + '_' + str(before) + '.csv'\n",
    "    df.to_csv('csv_exports' + '/' + csv_file_name, index = False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "https://api.pushshift.io/reddit/submission/search/?subreddit=france&after=1640905200&before=1643583600\n",
      "Error while accessing API\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'r' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\matth\\Documents\\Cours\\Algebra\\Cours\\Analytical techniques based on large data sets\\Project\\reddit_france_analysis\\scrapping\\reddit_scrap.ipynb Cell 4'\u001b[0m in \u001b[0;36mget_pushshift_data\u001b[1;34m(after, before, subreddit)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/Documents/Cours/Algebra/Cours/Analytical%20techniques%20based%20on%20large%20data%20sets/Project/reddit_france_analysis/scrapping/reddit_scrap.ipynb#ch0000003?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(URL)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/matth/Documents/Cours/Algebra/Cours/Analytical%20techniques%20based%20on%20large%20data%20sets/Project/reddit_france_analysis/scrapping/reddit_scrap.ipynb#ch0000003?line=4'>5</a>\u001b[0m r \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(URL)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/Documents/Cours/Algebra/Cours/Analytical%20techniques%20based%20on%20large%20data%20sets/Project/reddit_france_analysis/scrapping/reddit_scrap.ipynb#ch0000003?line=5'>6</a>\u001b[0m \u001b[39mif\u001b[39;00m r\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\reddit_scrap\\lib\\site-packages\\requests\\api.py:75\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/api.py?line=64'>65</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/api.py?line=65'>66</a>\u001b[0m \n\u001b[0;32m     <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/api.py?line=66'>67</a>\u001b[0m \u001b[39m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/api.py?line=71'>72</a>\u001b[0m \u001b[39m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/api.py?line=72'>73</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/api.py?line=74'>75</a>\u001b[0m \u001b[39mreturn\u001b[39;00m request(\u001b[39m'\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m'\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\reddit_scrap\\lib\\site-packages\\requests\\api.py:61\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/api.py?line=59'>60</a>\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/api.py?line=60'>61</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\reddit_scrap\\lib\\site-packages\\requests\\sessions.py:529\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/sessions.py?line=527'>528</a>\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/sessions.py?line=528'>529</a>\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/sessions.py?line=530'>531</a>\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\reddit_scrap\\lib\\site-packages\\requests\\sessions.py:645\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/sessions.py?line=643'>644</a>\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/sessions.py?line=644'>645</a>\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/sessions.py?line=646'>647</a>\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\reddit_scrap\\lib\\site-packages\\requests\\adapters.py:440\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/adapters.py?line=438'>439</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/adapters.py?line=439'>440</a>\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/adapters.py?line=440'>441</a>\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/adapters.py?line=441'>442</a>\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/adapters.py?line=442'>443</a>\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/adapters.py?line=443'>444</a>\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/adapters.py?line=444'>445</a>\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/adapters.py?line=445'>446</a>\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/adapters.py?line=446'>447</a>\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/adapters.py?line=447'>448</a>\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/adapters.py?line=448'>449</a>\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/adapters.py?line=449'>450</a>\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/adapters.py?line=450'>451</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/adapters.py?line=452'>453</a>\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/requests/adapters.py?line=453'>454</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\reddit_scrap\\lib\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=701'>702</a>\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=702'>703</a>\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=703'>704</a>\u001b[0m     conn,\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=704'>705</a>\u001b[0m     method,\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=705'>706</a>\u001b[0m     url,\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=706'>707</a>\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=707'>708</a>\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=708'>709</a>\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=709'>710</a>\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=710'>711</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=712'>713</a>\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=713'>714</a>\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=714'>715</a>\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=715'>716</a>\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\reddit_scrap\\lib\\site-packages\\urllib3\\connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=444'>445</a>\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=445'>446</a>\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=446'>447</a>\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=447'>448</a>\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=448'>449</a>\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=449'>450</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\reddit_scrap\\lib\\site-packages\\urllib3\\connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=442'>443</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=443'>444</a>\u001b[0m     httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=444'>445</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=445'>446</a>\u001b[0m     \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=446'>447</a>\u001b[0m     \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/site-packages/urllib3/connectionpool.py?line=447'>448</a>\u001b[0m     \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\reddit_scrap\\lib\\http\\client.py:1348\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/http/client.py?line=1346'>1347</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/http/client.py?line=1347'>1348</a>\u001b[0m     response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/http/client.py?line=1348'>1349</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\reddit_scrap\\lib\\http\\client.py:316\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/http/client.py?line=314'>315</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/http/client.py?line=315'>316</a>\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/http/client.py?line=316'>317</a>\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\reddit_scrap\\lib\\http\\client.py:277\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/http/client.py?line=275'>276</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/http/client.py?line=276'>277</a>\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/http/client.py?line=277'>278</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\reddit_scrap\\lib\\socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/socket.py?line=667'>668</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/socket.py?line=668'>669</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/socket.py?line=669'>670</a>\u001b[0m \u001b[39mexcept\u001b[39;00m timeout:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\reddit_scrap\\lib\\ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/ssl.py?line=1237'>1238</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/ssl.py?line=1238'>1239</a>\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/ssl.py?line=1239'>1240</a>\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/ssl.py?line=1240'>1241</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/ssl.py?line=1241'>1242</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\reddit_scrap\\lib\\ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/ssl.py?line=1097'>1098</a>\u001b[0m \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/ssl.py?line=1098'>1099</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   <a href='file:///c%3A/Users/matth/miniconda3/envs/reddit_scrap/lib/ssl.py?line=1099'>1100</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\matth\\Documents\\Cours\\Algebra\\Cours\\Analytical techniques based on large data sets\\Project\\reddit_france_analysis\\scrapping\\reddit_scrap.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/Documents/Cours/Algebra/Cours/Analytical%20techniques%20based%20on%20large%20data%20sets/Project/reddit_france_analysis/scrapping/reddit_scrap.ipynb#ch0000012?line=8'>9</a>\u001b[0m \u001b[39mwhile\u001b[39;00m nb_files_created \u001b[39m<\u001b[39m nb_months:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matth/Documents/Cours/Algebra/Cours/Analytical%20techniques%20based%20on%20large%20data%20sets/Project/reddit_france_analysis/scrapping/reddit_scrap.ipynb#ch0000012?line=9'>10</a>\u001b[0m     \u001b[39mif\u001b[39;00m nb_files_created \u001b[39m!=\u001b[39m nb_months \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/matth/Documents/Cours/Algebra/Cours/Analytical%20techniques%20based%20on%20large%20data%20sets/Project/reddit_france_analysis/scrapping/reddit_scrap.ipynb#ch0000012?line=10'>11</a>\u001b[0m         df \u001b[39m=\u001b[39m loop_between_dates(after,add_one_month(after), subreddit)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matth/Documents/Cours/Algebra/Cours/Analytical%20techniques%20based%20on%20large%20data%20sets/Project/reddit_france_analysis/scrapping/reddit_scrap.ipynb#ch0000012?line=11'>12</a>\u001b[0m         save_dataframe(df, after, add_one_month(after))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matth/Documents/Cours/Algebra/Cours/Analytical%20techniques%20based%20on%20large%20data%20sets/Project/reddit_france_analysis/scrapping/reddit_scrap.ipynb#ch0000012?line=12'>13</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32mc:\\Users\\matth\\Documents\\Cours\\Algebra\\Cours\\Analytical techniques based on large data sets\\Project\\reddit_france_analysis\\scrapping\\reddit_scrap.ipynb Cell 6'\u001b[0m in \u001b[0;36mloop_between_dates\u001b[1;34m(after, before, subreddit)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/Documents/Cours/Algebra/Cours/Analytical%20techniques%20based%20on%20large%20data%20sets/Project/reddit_france_analysis/scrapping/reddit_scrap.ipynb#ch0000005?line=2'>3</a>\u001b[0m rows_list \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/Documents/Cours/Algebra/Cours/Analytical%20techniques%20based%20on%20large%20data%20sets/Project/reddit_france_analysis/scrapping/reddit_scrap.ipynb#ch0000005?line=4'>5</a>\u001b[0m \u001b[39m#First call of the API with the original after / before intervals and the subreddit\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/matth/Documents/Cours/Algebra/Cours/Analytical%20techniques%20based%20on%20large%20data%20sets/Project/reddit_france_analysis/scrapping/reddit_scrap.ipynb#ch0000005?line=5'>6</a>\u001b[0m data \u001b[39m=\u001b[39m get_pushshift_data(after, before, subreddit)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/Documents/Cours/Algebra/Cours/Analytical%20techniques%20based%20on%20large%20data%20sets/Project/reddit_france_analysis/scrapping/reddit_scrap.ipynb#ch0000005?line=7'>8</a>\u001b[0m \u001b[39m#While our API calls are returning something, we keep scrapping\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/matth/Documents/Cours/Algebra/Cours/Analytical%20techniques%20based%20on%20large%20data%20sets/Project/reddit_france_analysis/scrapping/reddit_scrap.ipynb#ch0000005?line=8'>9</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32mc:\\Users\\matth\\Documents\\Cours\\Algebra\\Cours\\Analytical techniques based on large data sets\\Project\\reddit_france_analysis\\scrapping\\reddit_scrap.ipynb Cell 4'\u001b[0m in \u001b[0;36mget_pushshift_data\u001b[1;34m(after, before, subreddit)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matth/Documents/Cours/Algebra/Cours/Analytical%20techniques%20based%20on%20large%20data%20sets/Project/reddit_france_analysis/scrapping/reddit_scrap.ipynb#ch0000003?line=21'>22</a>\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matth/Documents/Cours/Algebra/Cours/Analytical%20techniques%20based%20on%20large%20data%20sets/Project/reddit_france_analysis/scrapping/reddit_scrap.ipynb#ch0000003?line=22'>23</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mError while accessing API\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/matth/Documents/Cours/Algebra/Cours/Analytical%20techniques%20based%20on%20large%20data%20sets/Project/reddit_france_analysis/scrapping/reddit_scrap.ipynb#ch0000003?line=23'>24</a>\u001b[0m     \u001b[39mprint\u001b[39m(r)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/matth/Documents/Cours/Algebra/Cours/Analytical%20techniques%20based%20on%20large%20data%20sets/Project/reddit_france_analysis/scrapping/reddit_scrap.ipynb#ch0000003?line=24'>25</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'r' referenced before assignment"
     ]
    }
   ],
   "source": [
    "after = int(datetime.timestamp(datetime(2021, 12, 31)))\n",
    "before = int(datetime.timestamp(datetime(2022, 3, 15)))\n",
    "subreddit = 'france'\n",
    "\n",
    "nb_files_created = 0\n",
    "nb_months = count_nb_months(after, before)\n",
    "print(nb_months)\n",
    "# Loop to make a file every month\n",
    "while nb_files_created < nb_months:\n",
    "    if nb_files_created != nb_months -1:\n",
    "        df = loop_between_dates(after,add_one_month(after), subreddit)\n",
    "        save_dataframe(df, after, add_one_month(after))\n",
    "    else:\n",
    "        df = loop_between_dates(after,before, subreddit)\n",
    "        save_dataframe(df, after, before)\n",
    "    after = add_one_month(after)\n",
    "    nb_files_created += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can't get score and comments datas\n",
    "Pushshift ingests comments once, in real time as soon as they are created. It doesn't update them afterwards. So at that time, the score is 1. There are various reasons that pushshift might have the score something other than 1, it was behind and ingested after it had been voted on, or at some point in the past there was a second ingest that updated comment scores 24 hours later. But neither of those are true for the beta api."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible solution\n",
    "If up to date data on things like score and num_comments etc is important, you can always combine Pushshift with the Reddit formal API via praw to get the best of both worlds. You use psaw and pushshift syntax to do things like retrieve large numbers of posts and/or specify date ranges, but the actual metadata for each post ID retrieved from Pushshift is then retrieved from Reddit itself.\n",
    "\n",
    "This combined method is slower than just using Pushshift, but if you want to just make one API call and make sure you get the up to date metadata, it works well. To do this though you do need reddit API creds. See here for code for combining psaw and praw.\n",
    "\n",
    "https://psaw.readthedocs.io/en/latest/#demo-usage-python\n",
    "\n",
    "One thing to be aware of though when looking at controversial things though... Pushshift retains all removed and deleted comments. So when you see a lot more reported comments via the Pushshift API than via Reddit itself, it's almost certainly because a lot of those comments were either removed or deleted. And, of course, if you try and retrieve those via the combination method I talked about above, you won't get anything because the code is trying to retrieve metadata for a post ID that exists on Pushshift but that doesn't exist on reddit itself anymore.\n",
    "\n",
    "Source : https://www.reddit.com/r/pushshift/comments/ofteoo/beta_api_inconsistencies_in_results/"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4cc03c265f4fdda4791346db1b93192099c83db4c485a5d9aa8708143edef8cd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('reddit_scrap')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
